{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from opencv-python) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from transformers import ViTImageProcessor, TrainingArguments, ViTForImageClassification, Trainer\n",
    "import torch \n",
    "\n",
    "from string import ascii_uppercase\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset, load_metric, ClassLabel\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Carregando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
      "Número de classes: 21\n"
     ]
    }
   ],
   "source": [
    "# Definir classes (letras do alfabeto, excluindo H, J, K, X e Z)\n",
    "classes = [letter for letter in ascii_uppercase if letter not in {'H', 'J', 'K', 'X', 'Z'}]\n",
    "\n",
    "num_classes = len(np.unique(classes))\n",
    "\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Número de classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(folder):\n",
    "        label_path = os.path.join(folder, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for filename in tqdm(os.listdir(label_path), desc=f\"Loading {label} images\"):\n",
    "                img_path = os.path.join(label_path, filename)\n",
    "                if img_path.endswith(\".jpg\") or img_path.endswith(\".png\"):\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "                    # Converter a matriz de volta para a imagem original\n",
    "                    image = Image.fromarray(img.astype('uint8'))\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading A images: 100%|██████████| 1686/1686 [00:00<00:00, 3496.47it/s]\n",
      "Loading B images: 100%|██████████| 1662/1662 [00:00<00:00, 3507.07it/s]\n",
      "Loading C images: 100%|██████████| 1686/1686 [00:00<00:00, 3512.50it/s]\n",
      "Loading D images: 100%|██████████| 1650/1650 [00:00<00:00, 3490.16it/s]\n",
      "Loading E images: 100%|██████████| 1670/1670 [00:00<00:00, 3450.41it/s]\n",
      "Loading F images: 100%|██████████| 1647/1647 [00:00<00:00, 3440.49it/s]\n",
      "Loading G images: 100%|██████████| 1650/1650 [00:00<00:00, 3444.68it/s]\n",
      "Loading I images: 100%|██████████| 1650/1650 [00:00<00:00, 3501.70it/s]\n",
      "Loading L images: 100%|██████████| 1650/1650 [00:00<00:00, 3503.19it/s]\n",
      "Loading M images: 100%|██████████| 1650/1650 [00:00<00:00, 3449.34it/s]\n",
      "Loading N images: 100%|██████████| 1650/1650 [00:00<00:00, 3495.76it/s]\n",
      "Loading O images: 100%|██████████| 1650/1650 [00:00<00:00, 3524.95it/s]\n",
      "Loading P images: 100%|██████████| 1650/1650 [00:00<00:00, 3481.01it/s]\n",
      "Loading Q images: 100%|██████████| 1650/1650 [00:00<00:00, 3479.25it/s]\n",
      "Loading R images: 100%|██████████| 1650/1650 [00:00<00:00, 3440.36it/s]\n",
      "Loading S images: 100%|██████████| 1650/1650 [00:00<00:00, 3398.60it/s]\n",
      "Loading T images: 100%|██████████| 1614/1614 [00:00<00:00, 3463.52it/s]\n",
      "Loading U images: 100%|██████████| 1650/1650 [00:00<00:00, 3486.68it/s]\n",
      "Loading V images: 100%|██████████| 1650/1650 [00:00<00:00, 3481.01it/s]\n",
      "Loading W images: 100%|██████████| 1649/1649 [00:00<00:00, 3499.63it/s]\n",
      "Loading Y images: 100%|██████████| 1650/1650 [00:00<00:00, 3503.19it/s]\n",
      "Loading A images: 100%|██████████| 579/579 [00:00<00:00, 3552.16it/s]\n",
      "Loading B images: 100%|██████████| 562/562 [00:00<00:00, 3490.69it/s]\n",
      "Loading C images: 100%|██████████| 583/583 [00:00<00:00, 3530.12it/s]\n",
      "Loading D images: 100%|██████████| 550/550 [00:00<00:00, 3525.66it/s]\n",
      "Loading E images: 100%|██████████| 574/574 [00:00<00:00, 3521.47it/s]\n",
      "Loading F images: 100%|██████████| 450/450 [00:00<00:00, 3515.62it/s]\n",
      "Loading G images: 100%|██████████| 550/550 [00:00<00:00, 3525.62it/s]\n",
      "Loading I images: 100%|██████████| 550/550 [00:00<00:00, 3481.02it/s]\n",
      "Loading L images: 100%|██████████| 550/550 [00:00<00:00, 3548.38it/s]\n",
      "Loading M images: 100%|██████████| 550/550 [00:00<00:00, 3520.00it/s]\n",
      "Loading N images: 100%|██████████| 550/550 [00:00<00:00, 3548.39it/s]\n",
      "Loading O images: 100%|██████████| 550/550 [00:00<00:00, 3536.18it/s]\n",
      "Loading P images: 100%|██████████| 550/550 [00:00<00:00, 3548.38it/s]\n",
      "Loading Q images: 100%|██████████| 550/550 [00:00<00:00, 3525.65it/s]\n",
      "Loading R images: 100%|██████████| 550/550 [00:00<00:00, 3525.64it/s]\n",
      "Loading S images: 100%|██████████| 550/550 [00:00<00:00, 3535.51it/s]\n",
      "Loading T images: 100%|██████████| 550/550 [00:00<00:00, 3481.01it/s]\n",
      "Loading U images: 100%|██████████| 550/550 [00:00<00:00, 3571.44it/s]\n",
      "Loading V images: 100%|██████████| 550/550 [00:00<00:00, 3548.39it/s]\n",
      "Loading W images: 100%|██████████| 550/550 [00:00<00:00, 3548.37it/s]\n",
      "Loading Y images: 100%|██████████| 550/550 [00:00<00:00, 3374.25it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_images, y_train_labels = load_images_from_folder('../libras_dataset/train')\n",
    "X_test_images, y_test_labels = load_images_from_folder('../libras_dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 34714\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 11548\n",
       " }))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = {\"image\": X_train_images, \"label\": y_train_labels}\n",
    "test_data = {\"image\": X_test_images, \"label\": y_test_labels}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, Value(dtype='string', id=None))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(train_dataset[\"label\"]))\n",
    "labels = train_dataset.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>,\n",
       " 'label': 'A'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwC2xqPdzTuxqFjz1pJXAVjUZrHOsvHeSRTY2KxAwORUP/CQkS4aFRHnqDzVukwubLHioiazBr0Rl2GIhScbs/0q6txFI21JUZvQGocGguPaoyKkpMVAGpuxULHnNSMagmfZGznoBmto7iZx2pPnUZD6uapyMQ340t27PIx5JJzVQlgRntW7EWi+SKdFM8cqtGxDDnNUxKd/PSnebiTjpiiwmdDp9/cSXISVyynjntWyeOc1ydlNm5jwSBuHFdXnjFYVlZ6DTNFjzUE8fmxOh/iGKmamZ60RdmNnFXkDW0pjbrVNkDV0uvWoeHzgPmU8/SuczXRuQVihBxSovNTHHekAFOwmzQ0uPddJ7HNdJmsjSYMAyEewrUzXNWd5WKiaRNJmgnmmk8UhkcgDKQwBB7GuW1WzFvOWQYVuRjtXUNWVqkYkt2PdRW1LV2Jkcz170q8HFMB65p6H5x9a1RDOmshttEx6VYziqto2LdRUxOa4p/EzRbH/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['label'] \n",
    "\n",
    "# labels.names[train_dataset[0]['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Rede com ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Modelo vit-base-patch16-224 (https://huggingface.co/google/vit-base-patch16-224) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"google/vit-base-patch16-224\"\n",
    "\n",
    "feature_extractor = ViTImageProcessor .from_pretrained(\n",
    "    model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.1216,  0.1216,  0.1216,  ..., -0.1529, -0.1529, -0.1529],\n",
       "          [ 0.1216,  0.1216,  0.1216,  ..., -0.1529, -0.1529, -0.1529],\n",
       "          [ 0.1216,  0.1216,  0.1216,  ..., -0.1529, -0.1529, -0.1529],\n",
       "          ...,\n",
       "          [-0.0196, -0.0196, -0.0196,  ..., -0.2235, -0.2235, -0.2235],\n",
       "          [-0.0196, -0.0196, -0.0196,  ..., -0.2235, -0.2235, -0.2235],\n",
       "          [-0.0196, -0.0196, -0.0196,  ..., -0.2235, -0.2235, -0.2235]],\n",
       "\n",
       "         [[ 0.1137,  0.1137,  0.1137,  ..., -0.1216, -0.1216, -0.1216],\n",
       "          [ 0.1137,  0.1137,  0.1137,  ..., -0.1216, -0.1216, -0.1216],\n",
       "          [ 0.1137,  0.1137,  0.1137,  ..., -0.1216, -0.1216, -0.1216],\n",
       "          ...,\n",
       "          [ 0.0039,  0.0039,  0.0039,  ..., -0.1608, -0.1608, -0.1608],\n",
       "          [ 0.0039,  0.0039,  0.0039,  ..., -0.1608, -0.1608, -0.1608],\n",
       "          [ 0.0039,  0.0039,  0.0039,  ..., -0.1608, -0.1608, -0.1608]],\n",
       "\n",
       "         [[ 0.0824,  0.0824,  0.0824,  ..., -0.2000, -0.2000, -0.2000],\n",
       "          [ 0.0824,  0.0824,  0.0824,  ..., -0.2000, -0.2000, -0.2000],\n",
       "          [ 0.0824,  0.0824,  0.0824,  ..., -0.2078, -0.2078, -0.2078],\n",
       "          ...,\n",
       "          [ 0.0196,  0.0196,  0.0196,  ..., -0.2471, -0.2471, -0.2471],\n",
       "          [ 0.0196,  0.0196,  0.0196,  ..., -0.2471, -0.2471, -0.2471],\n",
       "          [ 0.0196,  0.0196,  0.0196,  ..., -0.2471, -0.2471, -0.2471]]]])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = feature_extractor(\n",
    "    train_dataset['image'][0],\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape da Imagem original:  (64, 64) \n",
      "\n",
      "Shape da Imagem com resize do pytorch:  torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape da Imagem original: \", train_dataset['image'][0].size, \"\\n\")\n",
    "\n",
    "print(\"Shape da Imagem com resize do pytorch: \", example['pixel_values'].shape)\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Fine-tuning do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch): \n",
    "    inputs = feature_extractor(\n",
    "        batch['image'], \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs['label'] = batch['label']\n",
    "\n",
    "    return inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Adquirindo batchs de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = train_dataset.with_transform(preprocess)\n",
    "prepared_test = test_dataset.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions=p.predictions.argmax(axis=1),\n",
    "        references=p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Argumentos do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # logging_dir=\"./logs\",\n",
    "    # do_train=True,\n",
    "    # do_eval=True,\n",
    "    # metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Adquirindo modelo pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([21]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([21, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labels = classes\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Preparando o modelo pré-treinado para um novo treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_train,\n",
    "        eval_dataset=prepared_test,\n",
    "        tokenizer=feature_extractor\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) Novo Treinamento do modelo pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8680 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1837\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1839\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 4\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack([x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]),\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train.json\", train_results.metrics)\n",
    "trainer.save_metrics(\"train.json\", train_results.metrics)\n",
    "\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6) Testando modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(prepared_test)\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7) Visualizando predição com uma imagem especifica do conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "image = test_dataset['image'][0].resize((224, 224))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_label = test_dataset['label'][0]\n",
    "labels = test_dataset.features['label']\n",
    "\n",
    "actual_label, labels.names[actual_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(image)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
