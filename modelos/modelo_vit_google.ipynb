{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from opencv-python) (1.26.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m745.3/755.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: multiprocess in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: packaging in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pandas in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 12:51:10.454986: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 12:51:10.476018: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 12:51:10.476041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 12:51:10.476582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 12:51:10.480677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 12:51:10.945748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from transformers import ViTImageProcessor, TrainingArguments, ViTForImageClassification, Trainer\n",
    "import torch \n",
    "\n",
    "from string import ascii_uppercase\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset, load_metric, ClassLabel\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Carregando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
      "Número de classes: 21\n"
     ]
    }
   ],
   "source": [
    "# Definir classes (letras do alfabeto, excluindo H, J, K, X e Z)\n",
    "classes = [letter for letter in ascii_uppercase if letter not in {'H', 'J', 'K', 'X', 'Z'}]\n",
    "\n",
    "num_classes = len(np.unique(classes))\n",
    "\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Número de classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(folder):\n",
    "        label_path = os.path.join(folder, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for filename in tqdm(os.listdir(label_path), desc=f\"Loading {label} images\"):\n",
    "                img_path = os.path.join(label_path, filename)\n",
    "                if img_path.endswith(\".jpg\") or img_path.endswith(\".png\"):\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "                    # Converter a matriz de volta para a imagem original\n",
    "                    image = Image.fromarray(img.astype('uint8'))\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading C images: 100%|██████████| 1686/1686 [00:00<00:00, 10403.35it/s]\n",
      "Loading S images: 100%|██████████| 1650/1650 [00:00<00:00, 10755.16it/s]\n",
      "Loading T images: 100%|██████████| 1614/1614 [00:00<00:00, 10731.51it/s]\n",
      "Loading I images: 100%|██████████| 1650/1650 [00:00<00:00, 10496.14it/s]\n",
      "Loading A images: 100%|██████████| 1686/1686 [00:00<00:00, 10796.54it/s]\n",
      "Loading O images: 100%|██████████| 1650/1650 [00:00<00:00, 10915.71it/s]\n",
      "Loading E images: 100%|██████████| 1670/1670 [00:00<00:00, 10801.26it/s]\n",
      "Loading V images: 100%|██████████| 1650/1650 [00:00<00:00, 10545.74it/s]\n",
      "Loading L images: 100%|██████████| 1650/1650 [00:00<00:00, 10593.14it/s]\n",
      "Loading W images: 100%|██████████| 1649/1649 [00:00<00:00, 10064.37it/s]\n",
      "Loading D images: 100%|██████████| 1650/1650 [00:00<00:00, 10689.18it/s]\n",
      "Loading M images: 100%|██████████| 1650/1650 [00:00<00:00, 10636.43it/s]\n",
      "Loading P images: 100%|██████████| 1650/1650 [00:00<00:00, 10654.88it/s]\n",
      "Loading G images: 100%|██████████| 1650/1650 [00:00<00:00, 10628.19it/s]\n",
      "Loading F images: 100%|██████████| 1647/1647 [00:00<00:00, 10654.67it/s]\n",
      "Loading B images: 100%|██████████| 1662/1662 [00:00<00:00, 10576.17it/s]\n",
      "Loading U images: 100%|██████████| 1650/1650 [00:00<00:00, 10715.39it/s]\n",
      "Loading R images: 100%|██████████| 1650/1650 [00:00<00:00, 10609.47it/s]\n",
      "Loading Y images: 100%|██████████| 1650/1650 [00:00<00:00, 10717.15it/s]\n",
      "Loading Q images: 100%|██████████| 1650/1650 [00:00<00:00, 5888.73it/s]\n",
      "Loading N images: 100%|██████████| 1650/1650 [00:00<00:00, 10714.91it/s]\n",
      "Loading C images: 100%|██████████| 583/583 [00:00<00:00, 10700.36it/s]\n",
      "Loading S images: 100%|██████████| 550/550 [00:00<00:00, 10651.09it/s]\n",
      "Loading T images: 100%|██████████| 550/550 [00:00<00:00, 10529.55it/s]\n",
      "Loading I images: 100%|██████████| 550/550 [00:00<00:00, 10786.15it/s]\n",
      "Loading A images: 100%|██████████| 579/579 [00:00<00:00, 10606.11it/s]\n",
      "Loading O images: 100%|██████████| 550/550 [00:00<00:00, 10854.72it/s]\n",
      "Loading E images: 100%|██████████| 574/574 [00:00<00:00, 10668.98it/s]\n",
      "Loading V images: 100%|██████████| 550/550 [00:00<00:00, 10750.82it/s]\n",
      "Loading L images: 100%|██████████| 550/550 [00:00<00:00, 10738.91it/s]\n",
      "Loading W images: 100%|██████████| 550/550 [00:00<00:00, 10455.30it/s]\n",
      "Loading D images: 100%|██████████| 550/550 [00:00<00:00, 10772.96it/s]\n",
      "Loading M images: 100%|██████████| 550/550 [00:00<00:00, 10605.26it/s]\n",
      "Loading P images: 100%|██████████| 550/550 [00:00<00:00, 10869.96it/s]\n",
      "Loading G images: 100%|██████████| 550/550 [00:00<00:00, 10609.60it/s]\n",
      "Loading F images: 100%|██████████| 450/450 [00:00<00:00, 10634.41it/s]\n",
      "Loading B images: 100%|██████████| 562/562 [00:00<00:00, 10735.08it/s]\n",
      "Loading U images: 100%|██████████| 550/550 [00:00<00:00, 10757.94it/s]\n",
      "Loading R images: 100%|██████████| 550/550 [00:00<00:00, 9310.56it/s]\n",
      "Loading Y images: 100%|██████████| 550/550 [00:00<00:00, 10772.76it/s]\n",
      "Loading Q images: 100%|██████████| 550/550 [00:00<00:00, 10650.80it/s]\n",
      "Loading N images: 100%|██████████| 550/550 [00:00<00:00, 10492.53it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_images, y_train_labels = load_images_from_folder('libras_dataset/train')\n",
    "X_test_images, y_test_labels = load_images_from_folder('libras_dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 34714\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 11548\n",
       " }))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_labels = [classes.index(x) for x in y_train_labels]\n",
    "y_test_labels = [classes.index(x) for x in y_test_labels]\n",
    "train_data = {\"image\": X_train_images, \"label\": y_train_labels}\n",
    "test_data = {\"image\": X_test_images, \"label\": y_test_labels}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, Value(dtype='int64', id=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(train_dataset[\"label\"]))\n",
    "labels = train_dataset.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>,\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDrhS0UtaAJijFOxWRqGuR2smyIK+0/O5PAPoPU/wCfpUYOTshNpbmrijFYSeJkYZFs5H1x/Otizu4723WaPjPVe4NE6co7oSknsS4paXFGKzGIKWlxWRrGtx6cPKiw9yR93snuf8K1jFydkJuwmt6oLWI28L4nb7zA/cX1+vp+dclIQFDv0A+VewprytNK0szk5OWJ7mtDRbIatqQEi5toRucevoPxP6A12JKlEx1mzIfRfEV8hMFsxhkXch8xUBU9OprY8O+H9dtL9Zbu5NvAhBKBg3mDOdvB6cd/yNdv0pK5ZYictDZRSCiiisBnPan4kRMw2BDyd5f4R9PX+X1rkp5yrl5GLOTksTkk+9QNdBelV5pll716cYqCtE5m3J6iTXpdsKcD2r0DwdB5WimRgMyys2fUDA/oa87hgiLZZq6bSdfl0qPyAVkgJzg9R64NZ1ISnHQuDSZ31JXLP4xjByI0CjqC2TV7SfEtrq9wYI0ZHxlc8hq5ZUpRV2appm1RSZpM1mM8k0/QNX1NDJbwMYxxvZgo/DPX8Kvx+CdbaXaVijX++0gx+mT+lekxRRwQrFEgSNBhVHYU7Nbe0kLlRxEPw+corT6ltf8AiVIsgfQkj+VX4/AtgFAkvL1m9UdVH5bTXT0ZqXOXcfKjnrbwVpFvN5jm5uP9meQEfX5QP8K27eytLTJtrWCDIwfKjC5H4CpqKltvcBQaWm0bqkD/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['label'] \n",
    "\n",
    "# labels.names[train_dataset[0]['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Rede com ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Modelo vit-base-patch16-224 (https://huggingface.co/google/vit-base-patch16-224) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"google/vit-base-patch16-224\"\n",
    "\n",
    "feature_extractor = ViTImageProcessor .from_pretrained(\n",
    "    model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[0.3412, 0.3412, 0.3412,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.3412, 0.3412, 0.3412,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.3412, 0.3412, 0.3412,  ..., 0.3647, 0.3647, 0.3647],\n",
       "          ...,\n",
       "          [0.3569, 0.3569, 0.3569,  ..., 0.3490, 0.3490, 0.3490],\n",
       "          [0.3569, 0.3569, 0.3569,  ..., 0.3490, 0.3490, 0.3490],\n",
       "          [0.3569, 0.3569, 0.3569,  ..., 0.3490, 0.3490, 0.3490]],\n",
       "\n",
       "         [[0.3412, 0.3412, 0.3412,  ..., 0.3647, 0.3569, 0.3569],\n",
       "          [0.3412, 0.3412, 0.3412,  ..., 0.3647, 0.3569, 0.3569],\n",
       "          [0.3412, 0.3412, 0.3412,  ..., 0.3647, 0.3569, 0.3569],\n",
       "          ...,\n",
       "          [0.3647, 0.3647, 0.3647,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.3647, 0.3647, 0.3647,  ..., 0.3647, 0.3647, 0.3647],\n",
       "          [0.3647, 0.3647, 0.3647,  ..., 0.3647, 0.3647, 0.3647]],\n",
       "\n",
       "         [[0.4118, 0.4118, 0.4118,  ..., 0.3961, 0.3961, 0.3961],\n",
       "          [0.4118, 0.4118, 0.4118,  ..., 0.3961, 0.3961, 0.3961],\n",
       "          [0.4118, 0.4118, 0.4118,  ..., 0.3961, 0.3961, 0.3961],\n",
       "          ...,\n",
       "          [0.4353, 0.4353, 0.4353,  ..., 0.4118, 0.4118, 0.4118],\n",
       "          [0.4353, 0.4353, 0.4353,  ..., 0.4196, 0.4196, 0.4196],\n",
       "          [0.4353, 0.4353, 0.4353,  ..., 0.4196, 0.4196, 0.4196]]]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = feature_extractor(\n",
    "    train_dataset['image'][0],\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape da Imagem original:  (64, 64) \n",
      "\n",
      "Shape da Imagem com resize do pytorch:  torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape da Imagem original: \", train_dataset['image'][0].size, \"\\n\")\n",
    "\n",
    "print(\"Shape da Imagem com resize do pytorch: \", example['pixel_values'].shape)\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Fine-tuning do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch): \n",
    "    inputs = feature_extractor(\n",
    "        batch['image'], \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs['label'] = batch['label']\n",
    "\n",
    "    return inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Adquirindo batchs de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = train_dataset.with_transform(preprocess)\n",
    "prepared_test = test_dataset.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.pyenv/versions/3.10.12/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions=p.predictions.argmax(axis=1),\n",
    "        references=p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Argumentos do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=0.01,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # logging_dir=\"./logs\",\n",
    "    # do_train=True,\n",
    "    # do_eval=True,\n",
    "    # metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Adquirindo modelo pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([21]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([21, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labels = classes\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Preparando o modelo pré-treinado para um novo treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_train,\n",
    "        eval_dataset=prepared_test,\n",
    "        tokenizer=feature_extractor\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) Novo Treinamento do modelo pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebabca471be4c659b8396410b2e6298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4949, 'learning_rate': 0.00019976958525345624, 'epoch': 0.0}\n",
      "{'loss': 1.0546, 'learning_rate': 0.00019953917050691245, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/trainer.py:2781\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2781\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/accelerate/accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1964\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train.json\", train_results.metrics)\n",
    "trainer.save_metrics(\"train.json\", train_results.metrics)\n",
    "\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6) Testando modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(prepared_test)\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7) Visualizando predição com uma imagem especifica do conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "image = test_dataset['image'][0].resize((224, 224))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_label = test_dataset['label'][0]\n",
    "labels = test_dataset.features['label']\n",
    "\n",
    "actual_label, labels.names[actual_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(image)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
